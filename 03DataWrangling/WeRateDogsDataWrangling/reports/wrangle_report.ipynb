{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I wrangled (and analyzed and visualized) is the tweet archive of Twitter user @dog_rates, also known as [WeRateDogs](https://twitter.com/dog_rates). WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog.\n",
    "\n",
    "Real-world data rarely comes clean. Using Python and its libraries, I gathered data from a variety of sources and in a variety of formats. I assessed its quality and tidiness issues, then cleaned it. I documented the wrangling efforts in the `wrangle_act.ipynb` Jupyter Notebook, plus showcased them through analyses and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was gathered from three sources:\n",
    "\n",
    "Twitter archive: The WeRateDogs twitter archive. The file was provided by Udacity and it is donwloaded as `twitter_archive_enhanced.csv`. \n",
    "\n",
    "Image prediction: The `image_predictions.tsv` presents image prediction for each tweet predicted by a neural network. It is hosted on Udacity's servers. It is downloaded programitically using the [reuqest](https://requests.readthedocs.io/en/latest/) library.\n",
    "\n",
    "Twitter API: Using the tweet IDs in the Twitter archive, I gathered the data using [tweepy](https://www.tweepy.org/) library and stored it in the `tweet_json.txt` file. The gatherinng was proceeded in the Udacity Workspace to by pass the jupyter notebook security and company firewall issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data\n",
    "\n",
    "---\n",
    "\n",
    "I assessed both visually with Excel and programitically using Python. I summarized the following data quality and tidiness issues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality issues\n",
    "\n",
    "`rate_df` data frame\n",
    "* NaN values in the `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp`, `expanded_urls` columns. (**Completeness**)\n",
    "* `timestamp` should be datetime instead of string, if we want to retrieve information about time. (**Validity**)\n",
    "* `rating_numerator` is sometimes wrong for example at *832215726631055000* it is 75/10 instead of 9.75/10. Ratings numerators [0-1776] and denominators [0-170] are inconsistent range. Furthuremore the ratings are ambigous for example at *667878741721416000* 10&amp;2/10 rate and *695064344191721000* \"4/10 for Charles the puppy, 13/10 overall\". (**Accuracy**)\n",
    "* `rating_denominator` is inconsistent. Mostly is 10 but times when more dogs are on the picture it increased according to the number of dogs. (**Consistency**)\n",
    "* `source` is string, however it has four distinct values. It could be categorical (iphone, vine, web, tweetdeck) value. (**Consistency**)\n",
    "* `name` is a string and contains inaccurate names like 'a', 'an', 'O' due to the regular expression. (**Accuracy**)\n",
    "\n",
    "`predictions_df` data frame\n",
    "* `jpg_url` columns has duplicated values. Duplicated rows are next to each others and randomly distributed accross the dataset. (**Completeness**)\n",
    "* `p1_conf`, `p2_conf`, and `p3_conf` probabilites do not add up to one. (**Consistency**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness issues\n",
    "\n",
    "`rate_df` data frame\n",
    "* `doggo`, `floofer`, `pupper`, `puppo` are all dog types. It should be one column. (**Variable forms a column**)\n",
    "* `tweet_id` should be string instead of integer, because we want to merge them together instead of doing operations (add, substract, average) on them. (**Observational unit forms a table**)\n",
    "\n",
    "`predictions_df` data frame\n",
    "* `tweet_id` should be string instead of integer in order to merge the tables together. (**Observational unit forms a table**)\n",
    "* `img_num` column should be at the `rate_d` dataset, because it indicates how many image in the tweet. In other words how many links were in the `expanded_urls` columns seperated by commas. (**Observational unit forms a table**)\n",
    "\n",
    "`tweet_df` data frame\n",
    "* `tweet_id` should be string instead of integer. (**Observational unit forms a table**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data quality and tidiness problems mentioned above was cleaned by multiple methods including pandas join, regular expression, combining multiple columns, pandas subsetting, removing missing values and so on.\n",
    "\n",
    "After the cleaning, I stored the cleaned version of the data into a `twitter_archive_master.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe9a959b20c1a11f0379d800fd04c0320f2dff6d2d0003a40f8f375949ccb47d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
